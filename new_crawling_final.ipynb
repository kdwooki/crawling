{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as ur\n",
    "import random\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(good_list,bad_list,a_list):\n",
    "    count=0\n",
    "    pagenum=1\n",
    "    options=webdriver.ChromeOptions()\n",
    "    driver=webdriver.Chrome('./chromedriver')\n",
    "    driver.implicitly_wait(1)\n",
    "    #woman_shorts\n",
    "    driver.get('https://www.amazon.com/s?i=fashion-womens-intl-ship&bbn=16225018011&rh=n%3A16225018011%2Cn%3A1040660%2Cn%3A1048186&dc&pf_rd_i=16225018011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=0909843d-7d3a-40a7-8b35-6c3da8755bbd&pf_rd_p=0909843d-7d3a-40a7-8b35-6c3da8755bbd&pf_rd_r=W0B8SC0KBX7PVH85GSQQ&pf_rd_r=W0B8SC0KBX7PVH85GSQQ&pf_rd_s=merchandised-search-left-2&pf_rd_t=101&qid=1562351746&rnid=1040660&ref=sr_nr_n_8')\n",
    "    html=driver.page_source\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    while True:\n",
    "        #h=0\n",
    "        #id_list=[]\n",
    "        products=soup.findAll('a',attrs={'class':'a-link-normal a-text-normal'})\n",
    "        print(len(products))\n",
    "        gender='woman'\n",
    "        info={}\n",
    "        cnt=1\n",
    "        \n",
    "        #ids=soup.findAll('div',attrs={'class':'sg-col-4-of-24 sg-col-4-of-12 sg-col-4-of-36 sg-col-4-of-28 sg-col-4-of-16 sg-col sg-col-4-of-20 sg-col-4-of-32'})\n",
    "        #for j in range(48):\n",
    "          #  id=ids[j].get('data-asin')\n",
    "           # id_list.append(id)\n",
    "        #driver.implicitly_wait(7)\n",
    "    \n",
    "        minus=len(products)-48\n",
    "        print(minus)\n",
    "        if(minus>0):\n",
    "            for j in range(len(products)):\n",
    "                product=products[j].get('href')\n",
    "                if(j>=minus):   \n",
    "                    try:\n",
    "                        url=\"https://www.amazon.com\"+product\n",
    "                    #except:\n",
    "                      #  a_list.append(product)\n",
    "                        time.sleep(1)\n",
    "                        driver.get(url)\n",
    "                        html=driver.page_source\n",
    "                        soup=BeautifulSoup(html,'html.parser')\n",
    "                        time.sleep(1)\n",
    "                        title=soup.select('#productTitle')[0].text.strip()\n",
    "                        try:\n",
    "                #id=soup.select('#multipleRecommendations')[0].get('data-asin')\n",
    "                            id=soup.select('#cerberus-data-metrics')[0].get('data-asin')\n",
    "                        except:\n",
    "                            id='none'\n",
    "                        category_select = soup.select('#wayfinding-breadcrumbs_feature_div > ul > li > span')\n",
    "                        category = []\n",
    "                        for single_category in category_select:\n",
    "                            category.append(single_category.text.strip())\n",
    "    \n",
    "                        feature_select = soup.select('#feature-bullets > ul > li > span')\n",
    "                        feature = []\n",
    "                        for single_feature in feature_select:\n",
    "                            feature.append(single_feature.text.strip())\n",
    "                        imgurl_select = soup.select('#landingImage')[0]['data-a-dynamic-image']\n",
    "                        imgurl = (imgurl_select.split('\\\"')[1])\n",
    "                        h+=1\n",
    "                        if (id=='none'):\n",
    "                            info = {\n",
    "                        'id' : str(pagenum)+'-'+str(cnt),\n",
    "                    'title' : title,\n",
    "                    'category' : category,\n",
    "                    'features' : feature,\n",
    "                    'gender' : gender,\n",
    "                    'img_links' : imgurl\n",
    "                }\n",
    "                            bad_list.append(info)\n",
    "                            count+=1\n",
    "                        else:    \n",
    "                            info = {\n",
    "                    'id' : id,\n",
    "                    'title' : title,\n",
    "                    'category' : category,\n",
    "                    'features' : feature,\n",
    "                    'gender' : gender,\n",
    "                    'img_links' : imgurl\n",
    "                    }\n",
    "                            good_list.append(info)\n",
    "                            count+=1\n",
    "                        cnt+=1\n",
    "                        print(str(count)+'th')\n",
    "                        print(info)\n",
    "                #if (len(good_list)%1000==0):\n",
    "                   # json.dump(good_list,open('./newCrawling_womanShorts'+len(good_list)+'.json','w+'))\n",
    "            \n",
    "                        driver.back()\n",
    "                    except:\n",
    "                        if (j==47):\n",
    "                            a_list.append(product)\n",
    "                            a_last=driver.find_elements_by_class_name('a-last')[0]\n",
    "                            a_last.click()\n",
    "                        else:\n",
    "                            a_list.append(product)\n",
    "                    \n",
    "            a_last=driver.find_elements_by_class_name('a-last')[0]\n",
    "            a_last.click()\n",
    "            pagenum+=1\n",
    "            html=driver.page_source\n",
    "            soup=BeautifulSoup(html,'html.parser')\n",
    "            time.sleep(1)\n",
    "        \n",
    "        else:     \n",
    "            for h in range(len(products)):\n",
    "                if (h>=2):\n",
    "                    driver.get(url)\n",
    "                    html=driver.page_source\n",
    "                    soup=BeautifulSoup(html,'html.parser')\n",
    "                    time.sleep(1)\n",
    "                    title=soup.select('#productTitle')[0].text.strip()\n",
    "                    try:\n",
    "                    #id=soup.select('#multipleRecommendations')[0].get('data-asin')\n",
    "                        id=soup.select('#cerberus-data-metrics')[0].get('data-asin')\n",
    "                    except:\n",
    "                        id='none'\n",
    "                    category_select = soup.select('#wayfinding-breadcrumbs_feature_div > ul > li > span')\n",
    "                    category = []\n",
    "                    for single_category in category_select:\n",
    "                        category.append(single_category.text.strip())\n",
    "                        \n",
    "                    feature_select = soup.select('#feature-bullets > ul > li > span')\n",
    "                    feature = []\n",
    "                    for single_feature in feature_select:\n",
    "                        feature.append(single_feature.text.strip())\n",
    "                    imgurl_select = soup.select('#landingImage')[0]['data-a-dynamic-image']\n",
    "                    imgurl = (imgurl_select.split('\\\"')[1])\n",
    "                    h+=1\n",
    "                    if (id=='none'):\n",
    "                        info = {\n",
    "                        'id' : str(pagenum)+'-'+str(cnt),\n",
    "                    'title' : title,\n",
    "                    'category' : category,\n",
    "                    'features' : feature,\n",
    "                    'gender' : gender,\n",
    "                    'img_links' : imgurl\n",
    "                }\n",
    "                        bad_list.append(info)\n",
    "                        count+=1\n",
    "                    else:    \n",
    "                        info = {\n",
    "                    'id' : id,\n",
    "                    'title' : title,\n",
    "                    'category' : category,\n",
    "                    'features' : feature,\n",
    "                    'gender' : gender,\n",
    "                    'img_links' : imgurl\n",
    "                    }\n",
    "                        good_list.append(info)\n",
    "                        count+=1\n",
    "                    cnt+=1\n",
    "                    print(str(count)+'th')\n",
    "                    print(info)\n",
    "                if (len(good_list)%1000==0):\n",
    "                    json.dump(good_list,open('./newCrawling_womanShorts'+len(good_list)+'.json','w+'))\n",
    "            \n",
    "                    driver.back()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_list=[]\n",
    "bad_list=[]\n",
    "a_list=[]\n",
    "getFeature(good_list,bad_list,a_list)\n",
    "\n",
    "json.dump(good_list,open('./new_Crawling_womanShorts.json','w+'))\n",
    "json.dump(bad_list,open('./no_id_womanShorts.json','w+'))\n",
    "json.dump(a_list,open('./banList_womanShorts.json','w+'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
